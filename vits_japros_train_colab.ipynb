{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVlGtuUafliU"
      },
      "source": [
        "# VITS-JaPros-WebUI 訓練colabノートブック\n",
        "[VITS-JaPros-WebUI](https://github.com/litagin02/vits-japros-webui)の訓練パートのみを行うノートブックです。\n",
        "\n",
        "リポジトリのローカル使用では、`data`フォルダ（学習元データを入れるところ）と`outputs`フォルダ（途中結果や訓練結果等を入れるところ）がありますが、このノートブックではGoogle driveの`vits-japros-workspace`フォルダ内にこれらのフォルダを作って、そこで作業を行います。\n",
        "\n",
        "## 補足\n",
        "\n",
        "- 音声データのうち5ファイルは学習データとして使われず、検証データとして使われます。\n",
        "- どれだけの音声データがあれば質が良くなるか等は分かりません、実験してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9ZM_5kCgsfG"
      },
      "source": [
        "# 0. 環境構築\n",
        "今のcolab環境だとtensorflowの整合性エラーが出ることがあるみたいだけどたぶん問題ないはず。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zx5MxEQHQ82C"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/litagin02/vits-japros-webui\n",
        "%cd vits-japros-webui\n",
        "!pip install -r requirements.txt\n",
        "!curl -L \"https://huggingface.co/litagin/vits-japros-pretrained/resolve/main/pretrained.pth\" -o \"pretrained/pretrained.pth\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOSiPBqxOo4r"
      },
      "source": [
        " ## 1. データセットの準備\n",
        "- Google drive内の`vits-japros-workspace`フォルダで以下は作業します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S41i2YQPOIl5"
      },
      "outputs": [],
      "source": [
        "# Google driveのマウント\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97vhDYvSNRaM"
      },
      "outputs": [],
      "source": [
        "# フォルダ変数の設定\n",
        "import os\n",
        "\n",
        "root_dir = \"/content/drive/MyDrive/vits-japros-workspace/\"\n",
        "data_dir = os.path.join(root_dir, \"data\")\n",
        "wavs_dir = os.path.join(data_dir, \"wavs\")\n",
        "split_wavs_dir = os.path.join(data_dir, \"split_wavs\")\n",
        "output_dir = os.path.join(root_dir, \"outputs\")\n",
        "\n",
        "os.makedirs(wavs_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FypLwzgXaxpx"
      },
      "source": [
        "- 以下、フォルダは「Google drive内の`vits-japros-workspace`」を起点に考えてください。上のセルで、その中に`data/wavs/`フォルダがGoogle driveで作られたはずです。\n",
        "- `data/wavs/`フォルダ内に、学習に用いる音声wavファイルを入れてください。ファイル名は空白を含まない半角英数字にしてください。また**44.1kHzでモノラル**なことを前提とします。過程で自動的に変換されますが、その際に音質が落ちる可能性があります。\n",
        "\n",
        "- 既存コーパスなどでセリフ文章がすでにある場合は、`data/transcript_utf8.txt`ファイルに、音声ファイルのファイル名（拡張子以外）と、その音声のテキスト、半角コロン`:`区切りで書いてください。書き起こしが無い場合は、次のステップでの自動書き起こしを利用します。\n",
        "\n",
        "例：\n",
        "\n",
        "```txt\n",
        "wav_file1:これは最初の音声です。\n",
        "next_wav:これはもしかして、次のファイルの音声だったりする？\n",
        "third:そうかもしれないにゃー。\n",
        "...\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiGC7ziJPo75"
      },
      "source": [
        "## 2. 自動書き起こし\n",
        "\n",
        "`data/transcript_utf8.txt`を作成済みの場合は、このステップは不要です。\n",
        "\n",
        "[faster-whisper](https://github.com/guillaumekln/faster-whisper) を利用して、音声データから自動的に書き起こしを行います。\n",
        "\n",
        "2つのオプションがあります。\n",
        "\n",
        "1. 通常オプション: `data/wavs/`フォルダ内のwavファイル1つ1つをそのまま書き下します。\n",
        "2. 分割オプション: `data/wavs/`フォルダ内のwavファイルそれぞれを、文が区切れている箇所（`。`等）で区切って、音声ファイルも分割し`data/split_wavs`に保存し、書き下しも分割します。学習の精度が上がるかもしれませんが、**変な箇所で区切られて発話が途切れる可能性等もあります**。結果を確認することをおすすめします。\n",
        "\n",
        "結果は`data/transcript_utf8.txt`に上書き保存されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pyl1YZsHPpob"
      },
      "outputs": [],
      "source": [
        "# 通常オプションの場合\n",
        "!python transcribe.py --data-dir {data_dir}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzjKBDgZQ7xI"
      },
      "outputs": [],
      "source": [
        "# 分割オプションの場合\n",
        "!python transcribe_split.py --data-dir {data_dir}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lW_nOL7Uu0l"
      },
      "source": [
        "TIPS:\n",
        "\n",
        "余裕があれば、手動で音声ファイルを聞きながら`data/transcript_utf8.txt`を修正しましょう：\n",
        "- 不適切な音声（言葉にできない変な声・感情が激しすぎる声・途中で途切れている声等）があれば、その行を削除する（wavファイルはそのままで構いません）\n",
        "- 誤字脱字修正\n",
        "- 語尾が上がる疑問口調は、その箇所にちゃんと`？`を入れる\n",
        "- ポーズ位置に`、`や`。`を入れ、逆にポーズがないところには`、`等を削除する\n",
        "- 読み方が複数あって曖昧なものは、ひらがな等にする（「何で→なんで」「行った→おこなった」）\n",
        "\n",
        "が、こだわりすぎなくても大丈夫かもしれません、よく分かりません。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEvaQ5lsU8yL"
      },
      "source": [
        "## 3. 事前準備\n",
        "- `model_name`は、好きな名前（半角英数字）を指定してください。学習結果は`outputs/{model_name}/checkpoints/`フォルダ内に保存されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msYVXpniU9Q6"
      },
      "outputs": [],
      "source": [
        "model_name = \"test_model\"  # ここにモデル名を入力"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXidNx-HVErP"
      },
      "outputs": [],
      "source": [
        "# 通常オプションを使った場合\n",
        "!python preprocess.py --model-name {model_name} --data-dir {data_dir} --wavs-dir {wavs_dir} --output-dir {output_dir}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mb_xP0vlVZMD"
      },
      "outputs": [],
      "source": [
        "# 分割オプションを使った場合\n",
        "!python preprocess.py --model-name {model_name} --data-dir {data_dir} --wavs-dir {split_wavs_dir} --output-dir {output_dir}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMKckZIDWPiq"
      },
      "source": [
        "## 4. 学習\n",
        "\n",
        "- `model_name`は上で指定したものと同じものを指定してください。\n",
        "- `max_epoch`は、最大学習エポック数を指定します。デフォルトは200です。\n",
        "- `batch_bins`は、学習時のバッチサイズのようなものを指定します。デフォルトは1000000です。多いほどグラボのVRAM使用量は上がりますが、学習データ量によりどの程度まで上げられるかは変わるみたいです。グラボのVRAMに合わせてはみ出ないようにしてください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwWCNUJ2WYqp"
      },
      "outputs": [],
      "source": [
        "max_epoch = 200\n",
        "batch_bins = 1000000\n",
        "# model_name = \"test_model\"  # 3をスキップして途中から学習を再開するとき"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQcIqZQxkprF"
      },
      "source": [
        "下のセルを実行すると学習が開始されます（たぶん通常は終わらない）。\n",
        "\n",
        "- 結果は1エポックごとに`vits-japros-workspace/outputs/{model_name}/checkpoints/`の`{数字}epoch.pth`に保存されています。\n",
        "- 途中結果や学習に必要なデータはすべてGoogle driveの`vits-japros-workspace/outputs/`に保存されているので、途中から学習を再開するには、「0. 環境構築」と「1. データセットの準備」を行った後、そのまま「4. 学習」を実行できます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jj8qQCTaV8YM"
      },
      "outputs": [],
      "source": [
        "# 学習の実行\n",
        "!python train.py --model-name {model_name} --max-epoch {max_epoch} --batch-bins {batch_bins} --output-dir {output_dir}"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
